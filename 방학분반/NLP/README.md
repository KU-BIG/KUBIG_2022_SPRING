# Natural Language Processing
κ³ λ ¤λ€ν•™κµ λ°μ΄ν„° μ‚¬μ΄μ–Έμ¤ ν•™ν KUBIGμ—μ„ 2022λ…„ κ²¨μΈλ°©ν•™μ— μ§„ν–‰ν• μμ—°μ–΄μ²λ¦¬ μ¤ν„°λ””μ…λ‹λ‹¤. <br>
μμ—°μ–΄μ²λ¦¬ λ¶„λ°μ—μ„λ” κΈ°μ΄μ μΈ ν…μ¤νΈ λ°μ΄ν„°μ μ „μ²λ¦¬λ¶€ν„° μµμ‹ λ™ν–¥μ μ‚¬μ „ν•™μµλ¨λΈλ“¤μΈ BERTμ™€ GPTκΉμ§€ λ°°μΈ μ μλ” λ¶„λ°μ…λ‹λ‹¤. λ§¤μ£Ό μ΄λ΅  μ¤ν„°λ””λ¥Ό ν• λ’¤, λ°°μ΄ μ΄λ΅ μ— λ€ν•΄ μ½”λ“ κµ¬ν„κ³Όμ λ¥Ό ν•΄μ¤κ³  μ„λ΅ κ³µμ ν•λ” λ°©μ‹μΌλ΅ μ§„ν–‰ν–μµλ‹λ‹¤.
* λ¶„λ°μ¥: μΏ λΉ… 13κΈ° κΈ°λ‹¤μ—°
* λ¶„λ°μ›: μΏ λΉ… 12κΈ° μ›μ¤μ •, 14κΈ° μ κ°μλΉ, μ΄μ—°μ •, 15κΈ° κΉ€μ§„μ
* [**2022-Winter NLP Study Notion**](https://zoeylaboratory.notion.site/KUBIG-2022-Winter-NLP-b9658432b517473d82a224857b3015c2)μ—μ„ κ°μΈ λ³„ κ³Όμ  λ“± λ” μμ„Έν• μ¤ν„°λ”” μ§„ν–‰ λ‚΄μ©μ„ ν™•μΈν•μ‹¤ μ μμµλ‹λ‹¤. 

<br>

## π“ λ°©ν•™ μ¤ν„°λ”” μ§„ν–‰ μΌμ •

|   μ£Όμ°¨   |   μΌμ •   |   λ‚΄μ©   |   κ³Όμ  λ° λ…Όμ   | 
|:----------------------------|:----------------------------:|:--------------------:|:-------------------:|
|  1μ£Όμ°¨  | 2022.01.06 | **μ¤ν„°λ”” OT, μμ—°μ–΄μ²λ¦¬λ€?** | LSTM, RNN | 
|  2μ£Όμ°¨  | 2022.01.13 | **ν…μ¤νΈ λ°μ΄ν„°μ μ „μ²λ¦¬** | text preprocessing  | 
|  3μ£Όμ°¨  | 2022.01.20 | **μ–Έμ–΄ λ¨λΈ, λ²΅ν„°ν™”, word2vec** | gensim, word2vec (CBOW, Skipgram) κµ¬ν„ | 
|  4μ£Όμ°¨  | 2022.01.27 | **μ›λ“μ„λ² λ”©** | μ‚¬μ „ν•™μµλ μ›λ“μ„λ² λ”© | 
|  5μ£Όμ°¨  | 2022.02.10 | **Attention, Transformer** | Attention, Transformer |  
|  6μ£Όμ°¨  | 2022.02.17 | **μ‚¬μ „ν•™μµλ¨λΈ (BERT/GPT)** | BERT, GPT μ‹¤μµ |  
|  7μ£Όμ°¨  | 2022.02.24 | **KUBIG CONTEST μ¤€λΉ„** | KUBIG CONTEST μ¤‘κ°„λ°ν‘ |

<br>
