# Natural Language Processing
κ³ λ ¤λ€ν•™κµ λ°μ΄ν„° μ‚¬μ΄μ–Έμ¤ ν•™ν KUBIGμ—μ„ 2022λ…„ κ²¨μΈλ°©ν•™μ— μ§„ν–‰ν• μμ—°μ–΄μ²λ¦¬ μ¤ν„°λ””μ…λ‹λ‹¤.  
* λ¶„λ°μ¥: κΈ°λ‹¤μ—°
* [**2022-Winter NLP Study Notion**](https://zoeylaboratory.notion.site/KUBIG-2022-Winter-NLP-b9658432b517473d82a224857b3015c2)μ—μ„ κ°μΈ λ³„ κ³Όμ  λ“± λ” μμ„Έν• μ¤ν„°λ”” μ§„ν–‰ λ‚΄μ©μ„ ν™•μΈν•μ‹¤ μ μμµλ‹λ‹¤. 

<br>

## π“ λ°©ν•™ μ¤ν„°λ”” μ§„ν–‰ μΌμ •

|   μ£Όμ°¨   |   μΌμ •   |   λ‚΄μ©   |   κ³Όμ  λ° λ…Όμ   | 
|:----------------------------|:----------------------------:|:--------------------:|:-------------------:|
|  1μ£Όμ°¨  | 2022.01.06 | **μ¤ν„°λ”” OT, μμ—°μ–΄μ²λ¦¬λ€?** | LSTM, RNN | 
|  2μ£Όμ°¨  | 2022.01.13 | **ν…μ¤νΈ λ°μ΄ν„°μ μ „μ²λ¦¬** | text preprocessing  | 
|  3μ£Όμ°¨  | 2022.01.20 | **μ–Έμ–΄ λ¨λΈ, λ²΅ν„°ν™”, word2vec** | gensim, word2vec (CBOW, Skipgram) κµ¬ν„ | 
|  4μ£Όμ°¨  | 2022.01.27 | **μ›λ“μ„λ² λ”©** | μ‚¬μ „ν•™μµλ μ›λ“μ„λ² λ”© | 
|  5μ£Όμ°¨  | 2022.02.10 | **Attention, Transformer** | Attention, Transformer |  
|  6μ£Όμ°¨  | 2022.02.17 | **μ‚¬μ „ν•™μµλ¨λΈ (BERT/GPT)** | BERT, GPT μ‹¤μµ |  
|  7μ£Όμ°¨  | 2022.02.24 | **KUBIG CONTEST μ¤€λΉ„** | KUBIG CONTEST μ¤‘κ°„λ°ν‘ |

<br>
