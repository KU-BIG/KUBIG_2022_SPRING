{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "\bUnet_코드구현.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import transforms,datasets"
      ],
      "metadata": {
        "id": "rkfsKL7U7GgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 'nn.Module' 이라는 파이토치 base class를 상속받아서 \n",
        " # 사용자 정의 network 만들기\n",
        " # UNet class가 instance로 할당될때 초기화되는 함수 __init__, 이 함수에서 \n",
        " # 네트워크에 사용될 layer들을 전부 self.net으로 선언\n",
        " \n",
        "class UNet(nn.Module):  \n",
        "    def __init__(self): \n",
        "# super(subclass, self) : subclass에서 base class의 내용을 오버라이드해서 사용하고 싶을 때\n",
        "        super(UNet, self).__init__() \n",
        "        \n",
        "# 네트워크에서 반복적으로 사용되는 Conv + BatchNorm + Relu를 합쳐서 하나의 함수로 정의\n",
        "       \tdef CBR2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True):\n",
        "            layers = []\n",
        "            layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                                 kernel_size=kernel_size, stride=stride, padding=padding,\n",
        "                                 bias=bias)]\n",
        "            layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
        "            layers += [nn.ReLU()]\n",
        "\n",
        "            cbr = nn.Sequential(*layers) # *으로 list unpacking \n",
        "\n",
        "            return cbr\n",
        "\n",
        "        # Contracting path\n",
        "        self.enc1_1 = CBR2d(in_channels=1, out_channels=64)\n",
        "        self.enc1_2 = CBR2d(in_channels=64, out_channels=64)\n",
        "\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.enc2_1 = CBR2d(in_channels=64, out_channels=128)\n",
        "        self.enc2_2 = CBR2d(in_channels=128, out_channels=128)\n",
        "\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "  \n",
        "        # Expansive path\n",
        "        self.dec5_1 = CBR2d(in_channels=1024, out_channels=512)\n",
        "\n",
        "        self.unpool4 = nn.ConvTranspose2d(in_channels=512, out_channels=512,\n",
        "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
        "\n",
        "        self.dec4_2 = CBR2d(in_channels=2 * 512, out_channels=512)\n",
        "        self.dec4_1 = CBR2d(in_channels=512, out_channels=256)\n",
        "\n",
        "        self.unpool3 = nn.ConvTranspose2d(in_channels=256, out_channels=256,\n",
        "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
        "\n",
        "  \n",
        "# __init__ 함수에서 선언한 layer들 연결해서 data propa flow 만들기\n",
        "    def forward(self, x):\n",
        "        enc1_1 = self.enc1_1(x)\n",
        "        enc1_2 = self.enc1_2(enc1_1)\n",
        "        pool1 = self.pool1(enc1_2)\n",
        "\n",
        "        enc2_1 = self.enc2_1(pool1)\n",
        "        enc2_2 = self.enc2_2(enc2_1)\n",
        "        pool2 = self.pool2(enc2_2)\n",
        "\n",
        "        enc3_1 = self.enc3_1(pool2)\n",
        "        enc3_2 = self.enc3_2(enc3_1)\n",
        "        pool3 = self.pool3(enc3_2)\n",
        "\n",
        "        enc4_1 = self.enc4_1(pool3)\n",
        "        enc4_2 = self.enc4_2(enc4_1)\n",
        "        pool4 = self.pool4(enc4_2)\n",
        "\n",
        "        enc5_1 = self.enc5_1(pool4)\n",
        "\n",
        "        dec5_1 = self.dec5_1(enc5_1)\n",
        "\n",
        "        unpool4 = self.unpool4(dec5_1)\n",
        "        cat4 = torch.cat((unpool4, enc4_2), dim=1)\n",
        "        dec4_2 = self.dec4_2(cat4)\n",
        "        dec4_1 = self.dec4_1(dec4_2)\n",
        "\n",
        "        unpool3 = self.unpool3(dec4_1)\n",
        "        cat3 = torch.cat((unpool3, enc3_2), dim=1)\n",
        "        dec3_2 = self.dec3_2(cat3)\n",
        "        dec3_1 = self.dec3_1(dec3_2)\n",
        "\n",
        "        unpool2 = self.unpool2(dec3_1)\n",
        "        cat2 = torch.cat((unpool2, enc2_2), dim=1)\n",
        "        dec2_2 = self.dec2_2(cat2)\n",
        "        dec2_1 = self.dec2_1(dec2_2)\n",
        "\n",
        "        unpool1 = self.unpool1(dec2_1)\n",
        "        cat1 = torch.cat((unpool1, enc1_2), dim=1)\n",
        "        dec1_2 = self.dec1_2(cat1)\n",
        "        dec1_1 = self.dec1_1(dec1_2)\n",
        "\n",
        "        out = self.fc(dec1_1)\n",
        "\n",
        "        return out # data가 모든 layer를 거쳐서 나온 output 값"
      ],
      "metadata": {
        "id": "VRtq-tEjCVsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch.utils.data.Dataset): \n",
        "\n",
        "     # torch.utils.data.Dataset 이라는 파이토치 base class를 상속받아 \n",
        "     # 그 method인 __len__(), __getitem__()을 오버라이딩 해줘서 \n",
        "     # 사용자 정의 Dataset class를 선언한다\n",
        "     \n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        lst_data = os.listdir(self.data_dir)\n",
        "    \t\n",
        "        # 문자열 검사해서 'label'이 있으면 True \n",
        "        # 문자열 검사해서 'input'이 있으면 True\n",
        "        lst_label = [f for f in lst_data if f.startswith('label')] \n",
        "        lst_input = [f for f in lst_data if f.startswith('input')] \n",
        "        \n",
        "        lst_label.sort()\n",
        "        lst_input.sort()\n",
        "\n",
        "        self.lst_label = lst_label\n",
        "        self.lst_input = lst_input\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.lst_label)\n",
        "\t\n",
        "    # 여기가 데이터 load하는 파트\n",
        "    def __getitem__(self, index):\n",
        "        label = np.load(os.path.join(self.data_dir, self.lst_label[index]))\n",
        "        inputs = np.load(os.path.join(self.data_dir, self.lst_input[index]))\n",
        "\n",
        "# normalize, 이미지는 0~255 값을 가지고 있어 이를 0~1사이로 scaling\n",
        "        label = label/255.0\n",
        "        inputs = inputs/255.0\n",
        "        label = label.astype(np.float32)\n",
        "        inputs = inputs.astype(np.float32)\n",
        "        \n",
        "# 인풋 데이터 차원이 2이면, 채널 축을 추가해줘야한다. \n",
        "# 파이토치 인풋은 (batch, 채널, 행, 열)\n",
        "\n",
        "        if label.ndim == 2:  \n",
        "            label = label[:,:,np.newaxis]\n",
        "        if inputs.ndim == 2:  \n",
        "            inputs = inputs[:,:,np.newaxis] \n",
        "\n",
        "        data = {'input':inputs, 'label':label}\n",
        "\n",
        "        if self.transform:\t\t\t\t\n",
        "            data = self.transform(data)\n",
        "# transform에 할당된 class 들이 호출되면서 __call__ 함수 실행\n",
        "\n",
        "        return data"
      ],
      "metadata": {
        "id": "8CHaAvJ-CeuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ToTensor(object):\n",
        "    def __call__(self, data):\n",
        "        label, input = data['label'], data['input']\n",
        "\t\t\n",
        "        # numpy와 tensor의 배열 차원 순서가 다르다. \n",
        "        # numpy : (행, 열, 채널)\n",
        "        # tensor : (채널, 행, 열)\n",
        "        # 따라서 위 순서에 맞춰 transpose\n",
        "        \n",
        "        label = label.transpose((2, 0, 1)).astype(np.float32) \n",
        "        input = input.transpose((2, 0, 1)).astype(np.float32)\n",
        "\t\t\n",
        "        # 이후 np를 tensor로 바꾸는 코드는 다음과 같이 간단하다.\n",
        "        data = {'label': torch.from_numpy(label), 'input': torch.from_numpy(input)}\n",
        "\n",
        "        return data"
      ],
      "metadata": {
        "id": "mApW6FnsEzXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 하이퍼 파라미터 설정\n",
        "\n",
        "lr = 1e-3\n",
        "batch_size = 4\n",
        "num_epoch = 100\n",
        "\n",
        "data_dir = '/content/drive/My Drive/Colab Notebooks/파이토치/Architecture practice/UNet/data'\n",
        "ckpt_dir = '/content/drive/My Drive/Colab Notebooks/파이토치/Architecture practice/UNet/checkpoint'\n",
        "log_dir = '/content/drive/My Drive/Colab Notebooks/파이토치/Architecture practice/UNet/log'\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# transform 적용해서 데이터 셋 불러오기\n",
        "transform = transforms.Compose([Normalization(mean=0.5, std=0.5), RandomFlip(), ToTensor()])\n",
        "dataset_train = Dataset(data_dir=os.path.join(data_dir,'train'),transform=transform)\n",
        "\n",
        "# 불러온 데이터셋, 배치 size줘서 DataLoader 해주기\n",
        "loader_train = DataLoader(dataset_train, batch_size = batch_size, shuffle=True)\n",
        "\n",
        "# val set도 동일하게 진행\n",
        "dataset_val = Dataset(data_dir=os.path.join(data_dir,'val'),transform = transform)\n",
        "loader_val = DataLoader(dataset_val, batch_size=batch_size , shuffle=True)\n",
        "\n",
        "# 네트워크 불러오기\n",
        "net = UNet().to(device) # device : cpu or gpu\n",
        "\n",
        "# loss 정의\n",
        "fn_loss = nn.BCEWithLogitsLoss().to(device)\n",
        "\n",
        "# Optimizer 정의\n",
        "optim = torch.optim.Adam(net.parameters(), lr = lr ) \n",
        "\n",
        "# 기타 variables 설정\n",
        "num_train = len(dataset_train)\n",
        "num_val = len(dataset_val)\n",
        "\n",
        "num_train_for_epoch = np.ceil(num_train/batch_size) # np.ceil : 소수점 반올림\n",
        "num_val_for_epoch = np.ceil(num_val/batch_size)\n",
        "\n",
        "# 기타 function 설정\n",
        "fn_tonumpy = lambda x : x.to('cpu').detach().numpy().transpose(0,2,3,1) # device 위에 올라간 텐서를 detach 한 뒤 numpy로 변환\n",
        "fn_denorm = lambda x, mean, std : (x * std) + mean \n",
        "fn_classifier = lambda x :  1.0 * (x > 0.5)  # threshold 0.5 기준으로 indicator function으로 classifier 구현\n",
        "\n",
        "# Tensorbord\n",
        "writer_train = SummaryWriter(log_dir=os.path.join(log_dir,'train'))\n",
        "writer_val = SummaryWriter(log_dir = os.path.join(log_dir,'val'))"
      ],
      "metadata": {
        "id": "n-rhSkA4FIQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 네트워크 저장하기\n",
        "# train을 마친 네트워크 저장 \n",
        "# net : 네트워크 파라미터, optim  두개를 dict 형태로 저장\n",
        "def save(ckpt_dir,net,optim,epoch):\n",
        "    if not os.path.exists(ckpt_dir):\n",
        "        os.makedirs(ckpt_dir)\n",
        "\n",
        "    torch.save({'net':net.state_dict(),'optim':optim.state_dict()},'%s/model_epoch%d.pth'%(ckpt_dir,epoch))\n",
        "\n",
        "# 네트워크 불러오기\n",
        "def load(ckpt_dir,net,optim):\n",
        "    if not os.path.exists(ckpt_dir): # 저장된 네트워크가 없다면 인풋을 그대로 반환\n",
        "        epoch = 0\n",
        "        return net, optim, epoch\n",
        "    \n",
        "    ckpt_lst = os.listdir(ckpt_dir) # ckpt_dir 아래 있는 모든 파일 리스트를 받아온다\n",
        "    ckpt_lst.sort(key = lambda f : int(''.join(filter(str,isdigit,f))))\n",
        "\n",
        "    dict_model = torch.load('%s/%s' % (ckpt_dir,ckpt_lst[-1]))\n",
        "\n",
        "    net.load_state_dict(dict_model['net'])\n",
        "    optim.load_state_dict(dict_model['optim'])\n",
        "    epoch = int(ckpt_lst[-1].split('epoch')[1].split('.pth')[0])\n",
        "\n",
        "    return net,optim,epoch\n",
        "\n",
        "\n",
        "# 네트워크 학습시키기\n",
        "start_epoch = 0\n",
        "net, optim, start_epoch = load(ckpt_dir = ckpt_dir, net = net, optim = optim) # 저장된 네트워크 불러오기\n",
        "\n",
        "for epoch in range(start_epoch+1,num_epoch +1):\n",
        "    net.train()\n",
        "    loss_arr = []\n",
        "\n",
        "    for batch, data in enumerate(loader_train,1): # 1은 뭐니 > index start point\n",
        "        # forward\n",
        "        label = data['label'].to(device)   # 데이터 device로 올리기     \n",
        "        inputs = data['input'].to(device)\n",
        "        output = net(inputs) \n",
        "\n",
        "        # backward\n",
        "        optim.zero_grad()  # gradient 초기화\n",
        "        loss = fn_loss(output, label)  # output과 label 사이의 loss 계산\n",
        "        loss.backward() # gradient backpropagation\n",
        "        optim.step() # backpropa 된 gradient를 이용해서 각 layer의 parameters update\n",
        "\n",
        "        # save loss\n",
        "        loss_arr += [loss.item()]\n",
        "\n",
        "        # tensorbord에 결과값들 저정하기\n",
        "        label = fn_tonumpy(label)\n",
        "        inputs = fn_tonumpy(fn_denorm(inputs,0.5,0.5))\n",
        "        output = fn_tonumpy(fn_classifier(output))\n",
        "\n",
        "        writer_train.add_image('label', label, num_train_for_epoch * (epoch - 1) + batch, dataformats='NHWC')\n",
        "        writer_train.add_image('input', inputs, num_train_for_epoch * (epoch - 1) + batch, dataformats='NHWC')\n",
        "        writer_train.add_image('output', output, num_train_for_epoch * (epoch - 1) + batch, dataformats='NHWC')\n",
        "\n",
        "    writer_train.add_scalar('loss', np.mean(loss_arr), epoch)\n",
        "\n",
        "    \n",
        "    # validation\n",
        "    with torch.no_grad(): # validation 이기 때문에 backpropa 진행 x, 학습된 네트워크가 정답과 얼마나 가까운지 loss만 계산\n",
        "        net.eval() # 네트워크를 evaluation 용으로 선언\n",
        "        loss_arr = []\n",
        "\n",
        "        for batch, data in enumerate(loader_val,1):\n",
        "            # forward\n",
        "            label = data['label'].to(device)\n",
        "            inputs = data['input'].to(device)\n",
        "            output = net(inputs)\n",
        "\n",
        "            # loss \n",
        "            loss = fn_loss(output,label)\n",
        "            loss_arr += [loss.item()]\n",
        "            print('valid : epoch %04d / %04d | Batch %04d \\ %04d | Loss %04d'%(epoch,num_epoch,batch,num_val_for_epoch,np.mean(loss_arr)))\n",
        "\n",
        "            # Tensorboard 저장하기\n",
        "            label = fn_tonumpy(label)\n",
        "            inputs = fn_tonumpy(fn_denorm(inputs, mean=0.5, std=0.5))\n",
        "            output = fn_tonumpy(fn_classifier(output))\n",
        "\n",
        "            writer_val.add_image('label', label, num_val_for_epoch * (epoch - 1) + batch, dataformats='NHWC')\n",
        "            writer_val.add_image('input', inputs, num_val_for_epoch * (epoch - 1) + batch, dataformats='NHWC')\n",
        "            writer_val.add_image('output', output, num_val_for_epoch * (epoch - 1) + batch, dataformats='NHWC')\n",
        "\n",
        "        writer_val.add_scalar('loss', np.mean(loss_arr), epoch)\n",
        "\n",
        "        # epoch이 끝날때 마다 네트워크 저장\n",
        "        save(ckpt_dir=ckpt_dir, net = net, optim = optim, epoch = epoch)\n",
        "\n",
        "writer_train.close()\n",
        "writer_val.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "t21-hWavE3_n",
        "outputId": "7dc557a9-35a9-414d-b8eb-8663b14e8c33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-68762cd7626e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# 네트워크 학습시키기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mstart_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mckpt_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 저장된 네트워크 불러오기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epoch\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ckpt_dir' is not defined"
          ]
        }
      ]
    }
  ]
}